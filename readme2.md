Hereâ€™s your updated README with Celery integration added seamlessly.

Django Web Crawler with Docker

This project is a Django-based web crawler that uses PostgreSQL, containerized with Docker and secured using Docker Secrets.

ðŸ“Œ Table of Contents
	â€¢	ðŸš€ Setup Instructions
	â€¢	âš™ï¸ Celery & Task Scheduling
	â€¢	ðŸ·ï¸ Category Structure
	â€¢	âš¡ Performance Optimizations
	â€¢	ðŸ“œ API Documentation
	â€¢	API Endpoints (Router-based)
	â€¢	HTML-Based Views (Template Views)
	â€¢	ðŸ“Š Logging & Debugging

ðŸš€ Setup Instructions

1ï¸âƒ£ Initialize Docker Swarm

Before using Docker Secrets, initialize Swarm mode (required for secrets):

docker swarm init

2ï¸âƒ£ Create Database Secrets

Run the following commands to securely store database credentials:

echo "$DATABASE_NAME" | docker secret create db_name -
echo "$DATABASE_USER" | docker secret create db_user -
echo "$DATABASE_PASSWORD" | docker secret create db_password -
echo "$DATABASE_HOST" | docker secret create db_host -
echo "$DATABASE_PORT" | docker secret create db_port -

To verify the secrets:

docker secret ls

3ï¸âƒ£ Manually Build the Docker Image

Docker Swarm does not support build: in docker-compose.yml, so you need to build the image manually.

docker build -t django-crawler .

4ï¸âƒ£ Deploy the Application

Run the following command to start Django, PostgreSQL, Redis, and Celery using Docker Swarm:

docker stack deploy -c docker-compose.yml django-crawler

5ï¸âƒ£ Managing the Database

Run database migrations inside the container:

docker exec -it $(docker ps -q --filter name=web) poetry run python manage.py migrate

Create database migrations:

docker exec -it $(docker ps -q --filter name=web) poetry run python manage.py makemigrations

Create a superuser for Django Admin:

docker exec -it $(docker ps -q --filter name=web) poetry run python manage.py createsuperuser

6ï¸âƒ£ Verify the Running Containers

Check that all services are running:

docker ps

Expected output:

CONTAINER ID   IMAGE              STATUS         PORTS
123abc456def   django-crawler     Up 2 minutes   0.0.0.0:8000->8000/tcp
789xyz123ghi   postgres:16        Up 2 minutes   0.0.0.0:5432->5432/tcp
456def789xyz   redis:latest       Up 2 minutes   0.0.0.0:6379->6379/tcp

7ï¸âƒ£ How to Stop the Application

To stop and remove all services:

docker stack rm django-crawler

To remove unused volumes and networks:

docker system prune -a

âš™ï¸ Celery & Task Scheduling

1ï¸âƒ£ Install Redis (Required for Celery)

Ensure that Redis is installed and running.

For macOS (Homebrew)

brew install redis
brew services start redis

For Linux (Debian/Ubuntu)

sudo apt update && sudo apt install redis-server
sudo systemctl start redis
sudo systemctl enable redis

For Windows (via WSL)

Use Docker or WSL to run Redis:

docker run -d --name redis -p 6379:6379 redis

2ï¸âƒ£ Start Celery Worker

Run the following command to start a Celery worker:

poetry run celery -A config worker --loglevel=info

This starts Celery in worker mode, listening for tasks.

3ï¸âƒ£ Running Celery Beat (Scheduled Tasks)

To schedule recurring tasks (e.g., automatic scraping), start Celery Beat:

poetry run celery -A config beat --loglevel=info

4ï¸âƒ£ Verify Celery is Running

Check active tasks:

poetry run celery -A config inspect active
poetry run celery -A config inspect scheduled
poetry run celery -A config inspect reserved

If no tasks are scheduled, recheck Redis:

redis-cli ping  # Should return PONG

5ï¸âƒ£ Manually Trigger Web Scraping

To manually test the scraper, enter Django shell:

poetry run python manage.py shell

Then run:

from apps.crawler_app.tasks import scrape_products
scrape_products.delay()

Check Celery logs for execution.

6ï¸âƒ£ Dockerizing Celery (For Production)

Ensure Celery is running inside Docker for deployments:
	1.	Build the Celery image manually:

docker build -t django-crawler-celery .


	2.	Deploy it as a stack:

docker stack deploy -c docker-compose.yml django-crawler


	3.	Verify services:

docker service ls

âœ… Celery in Django Admin
	1.	Open Django Admin (/admin/django_celery_beat/periodictask/)
	2.	Add a new periodic task:
	â€¢	Task Name: scrape_products
	â€¢	Interval: Choose Every X minutes
	3.	Save and start scheduling tasks automatically.

ðŸ·ï¸ Category Structure
	â€¢	Categories are now hierarchical, supporting unlimited depth (e.g., decoration > bedroom > bed).
	â€¢	The system automatically creates missing parent categories when a new category is added.

Example:

Category Path	Parent
decoration > bedroom > bed	decoration > bedroom
office-furniture > table > conference-table	office-furniture > table

# Fetch all categories via API
curl -X GET http://localhost:8000/api/categories/

ðŸ“œ API Documentation

API Endpoints (Router-based)

These are registered with Django REST frameworkâ€™s DefaultRouter, meaning they provide CRUD operations automatically.

Endpoint	ViewSet	Description
/api/products/	ProductViewSet	List and manage products (CRUD)
/api/categories/	CategoryViewSet	List and manage categories (CRUD)

ðŸ“Š Logging & Debugging

Logs are stored in logs/ and can be accessed via:

# View the latest logs
tail -f logs/django.log

# View the latest warnings and errors only
tail -f logs/django_warn.log

This helps debugging and makes log access clear.

ðŸŽ¯ Next Steps

Would you like to:
	â€¢	Optimize query indexing for PostgreSQL?
	â€¢	Implement caching with Redis to speed up repeated queries?
	â€¢	Improve API response times using Django REST Framework optimizations?
	â€¢	Add automated testing with Pytest and FactoryBoy?

âœ… Celery is now fully documented in your README! ðŸŽ‰
ðŸ”¹ Next up: Writing automated tests. Would you like to begin with unit tests or integration tests? ðŸš€